{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb806bad-5a51-47e9-8d14-98d35e4291d0",
   "metadata": {},
   "source": [
    "# QA Retreival \n",
    "\n",
    "https://colab.research.google.com/drive/1zG1R08TBikG05ecF8et4vi_1F9xutY-6?usp=sharing#scrollTo=jVWgPJXs1yRq\n",
    "\n",
    "Local Model used is google/flan-t5-xl \n",
    "\n",
    "- https://huggingface.co/google/flan-t5-xl\n",
    "\n",
    "\n",
    "Local Embeddings Used\n",
    "\n",
    "-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de21fe87-9988-4763-b699-f6a524fb01fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip -q install langchain tiktoken chromadb pypdf transformers InstructorEmbedding\n",
    "# !pip -q install accelerate bitsandbytes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad3e5c3-ba02-475f-834b-5102cd7e22f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -q https://www.dropbox.com/s/zoj9rnm7oyeaivb/new_papers.zip\n",
    "# !unzip -q new_papers.zip -d new_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464a1a75-cd3c-4cb6-bcc2-827269815c40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# WORKS\n",
    "\n",
    "# import torch\n",
    "# import transformers\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "# # torch_dtype=torch.float16, low_cpu_mem_usage=True,\n",
    "                                            \n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\",\n",
    "#                                               load_in_8bit=False,\n",
    "#                                               device_map='cuda:0'\n",
    "#                                               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b45ad83-f702-4a1a-8e2c-ae7dfe5daaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Writer/palmyra-base\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Writer/palmyra-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27110f02-045a-4bba-92d5-df506c3b35eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOT WORKING\n",
    "\n",
    "# import torch\n",
    "# import transformers\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xl\",\n",
    "#                                               load_in_4bit=True,\n",
    "#                                               device_map='auto',\n",
    "#                                             #   torch_dtype=torch.float16,\n",
    "#                                             #   low_cpu_mem_usage=True,\n",
    "                                            \n",
    "#                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca1da5b-22e2-44e0-bc9c-2337e6cadcf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d068d9-4ee6-4323-86ed-8d3d17689d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "api_key = open('./hf_key.txt').read()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=api_key)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ecf594-9d54-4211-b47c-676d20619841",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdc60b4-2de1-4a76-92e2-11edef7c9c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=2048,\n",
    "    temperature=0,\n",
    "    repetition_penalty=1.15,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# pipe = pipeline(\n",
    "#     # \"text2text-generation\",\n",
    "#     \"text-generation\",\n",
    "#     model=model, \n",
    "#     #tokenizer=tokenizer, \n",
    "#     max_length=2048,\n",
    "#     temperature=0,\n",
    "#     # top_p=0.95,\n",
    "#     repetition_penalty=1.15,\n",
    "#     # batch_size=16\n",
    "# )\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=\"TheBloke/falcon-40b-instruct-GPTQ\", trust_remote_code=True,\n",
    "#                max_length=2048,\n",
    "#                 temperature=0,\n",
    "#                 # top_p=0.95,\n",
    "#                 repetition_penalty=1.15)\n",
    "\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35b6b17-ba29-41c4-9633-d0036251e077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# pipe2 = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "# local_llm2 = HuggingFacePipeline(pipeline=pipe2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ead41d-08b8-4d21-ab3b-4f4fb0af34bd",
   "metadata": {},
   "source": [
    "Check is LLM works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eb999e-7d6f-462b-a05c-6879936a0f42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(local_llm.invoke('What is the capital of the canada?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7551677e-6630-47ba-a03a-00a0ddb7a000",
   "metadata": {},
   "source": [
    "# LangChain multi-doc retriever with ChromaDB\n",
    "\n",
    "***New Points***\n",
    "- Multiple Files - PDFs\n",
    "- ChromaDB\n",
    "- Local LLM\n",
    "- Instuctor Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfa9be5-5356-45b0-96ab-0b138c3bd64d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setting up LangChain \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d83b2c-e724-4875-a5a6-f161ef70800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ff9695-e889-404e-aa98-ce9c935630ba",
   "metadata": {},
   "source": [
    "## Load multiple and process documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b24a6be-0cfd-4716-8101-45ea787c431c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load and process the text files\n",
    "# loader = TextLoader('single_text_file.pdf')\n",
    "loader = DirectoryLoader('./new_papers/', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d9c108-2f85-4c41-b749-9cbc907fb9b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd309d2-1537-4b20-8156-7d5c63bfa375",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#splitting the text into\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48e4f15-e6f9-4293-bbc8-0e8b80f8af05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce260514-a5a4-433a-9aca-0bf849e7a30c",
   "metadata": {},
   "source": [
    "## HF Instructor Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6d5dd6-9b30-404f-87c6-3aaac1fdd6df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings, HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cuda:1\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\", \n",
    "                                                      # model_kwargs={\"device\": \"cuda:1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e599029-0d13-41e9-ac36-82f00d218253",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Embed and store the texts\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "persist_directory = 'news_papers/db'\n",
    "\n",
    "## Here is the nmew embeddings being used\n",
    "embedding = hf\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=texts, \n",
    "                                 embedding=embedding,\n",
    "                                 persist_directory=persist_directory)\n",
    "\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166bfb9f-b9fa-4328-9047-383aa8398243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ab21d1-fe30-4c71-9ae3-c5fb0c0b90d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the chain to answer questions \n",
    "qa_chain = RetrievalQA.from_chain_type(llm=local_llm, \n",
    "                                  chain_type=\"refine\", \n",
    "                                  retriever=retriever, \n",
    "                                  return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671777ff-b5ce-4ae4-884d-a2275da2c36e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Cite sources\n",
    "\n",
    "import textwrap\n",
    "\n",
    "def wrap_text_preserve_newlines(text, width=110):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39427ebc-da5a-4dd3-9fcf-c741e7a18636",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# full example\n",
    "query = \"What is Flash attention?\"\n",
    "llm_response = qa_chain.invoke(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3243a0a0-1fd5-4b12-8c34-0c75af63190e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What does IO-aware mean?\"\n",
    "llm_response = qa_chain.invoke(query)\n",
    "process_llm_response(llm_response)\n",
    "# llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e9854-8319-43a9-8c3b-42b88fd5d9d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is tiling in flash-attention?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56c30eb-d2df-4f40-8ee1-fc3de7c27206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is toolformer?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3418094e-bb54-47f2-9039-c7c9527fd968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"How many examples do we need to provide for each tool?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147c001e-d3b3-493d-9517-023657fa77ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What are the best retrieval augmentations for LLMs?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df5bc7-da45-4cb5-82a8-57e769be3a10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What are the differences between REALM and RAG?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792cc42-917f-4d9c-948b-28280168d26d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd5ab88-103a-4e7c-b069-32ad86fb44e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8a8225-d31c-4f06-9459-e94ddd7df3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
