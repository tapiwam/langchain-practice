{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64959efd",
   "metadata": {},
   "source": [
    "# Lanchain OpenAI Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "254ea1fe-8746-4c51-9d8e-c4c1aaa1eb5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9faf781b-16dd-4f62-9922-82ee6940461f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain.cache import SQLiteCache\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "\n",
    "cacheType = 'in_memory'\n",
    "\n",
    "if cacheType == 'in_memory':\n",
    "    set_llm_cache(InMemoryCache())\n",
    "elif cacheType == 'sqlite':\n",
    "    set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f511ab0b-6bc3-4b5f-b627-0e28662a0f0a",
   "metadata": {},
   "source": [
    "# Setup LLM - Google Flan T5 Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f03d9dc8-8bad-4820-a585-23c3e82ce3a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import transformers\n",
    "# from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "# # torch_dtype=torch.float16, low_cpu_mem_usage=True,\n",
    "                                            \n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\",\n",
    "#                                               load_in_8bit=False,\n",
    "#                                               device_map='cuda:0'\n",
    "#                                               )\n",
    "\n",
    "# pipe = pipeline(\n",
    "#     \"text2text-generation\",\n",
    "#     model=model, \n",
    "#     tokenizer=tokenizer, \n",
    "#     max_length=2048,\n",
    "#     temperature=0,\n",
    "#     repetition_penalty=1.15,\n",
    "#     batch_size=10\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec68a793-d68c-47f8-acde-b2c004e40546",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin C:\\Users\\Tapiwa\\AppData\\Roaming\\Python\\Python311\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118_nocublaslt.dll\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ac07352567416aa47792631840a06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"TheBloke/wizardLM-7B-HF\")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\"TheBloke/wizardLM-7B-HF\",\n",
    "                                              load_in_4bit=True,\n",
    "                                              device_map='cuda:0',\n",
    "                                              torch_dtype=torch.float16,\n",
    "                                              low_cpu_mem_usage=True\n",
    "                                              )\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=1024,\n",
    "    temperature=0,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a3313f5-24c4-49f4-9bff-5e6fdb0cfe37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8c160a4-2506-4dda-83d2-c29d54b0949b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tapiwa\\AppData\\Roaming\\Python\\Python311\\site-packages\\bitsandbytes\\nn\\modules.py:224: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'?\\nCanada does not have a capital city. Instead, Ottawa is the capital of Canada and serves as the seat of government for both the federal and provincial/territorial governments.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat('What is the capital on Canada')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b486dcf4-23e8-4ffb-a5fa-781f7c9f91d7",
   "metadata": {},
   "source": [
    "## HF Instructor Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54e2518e-70b5-46f9-9150-7c6ebee2bad0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceInstructEmbeddings, HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cuda:1\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embedding = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a97e058-a20b-488c-ab7b-952f580fbaaa",
   "metadata": {},
   "source": [
    "# Load Multiple documents and process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eab49119-15b2-47bb-910a-6f99ef888de4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load all files in dirl\n",
    "loader = DirectoryLoader('./new_articles/', glob=\"./*.txt\", loader_cls=TextLoader)\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33c721f-3fc5-4a6d-a39b-231c85ab1e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ac7013d-9c1a-41c5-a3ad-d3512f9260b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# Import chat templates\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7729c05c-812b-4389-825e-89357f0c9a30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "chain = load_summarize_chain(llm=chat, chain_type=\"refine\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff13081-841f-4348-96bd-8d5855e28b17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e2768f32-2298-476c-af55-7307eefdc23b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1090: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents([documents[0]])\n",
    "\n",
    "chain.run(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94000a34-11a0-44bf-b1bc-a48cd11d97b4",
   "metadata": {},
   "source": [
    "## Pydantic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3f1471-deb6-46c2-9124-0a0183f01071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from langchain.output_parsers import PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d45a24-5e70-413a-8d88-9bea689dd372",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAModel(BaseModel):\n",
    "    question: str = Field(description=\"Question\")\n",
    "    answer: str = Field(description=\"Answer to question\")\n",
    "\n",
    "class QADocument(BaseModel):\n",
    "    metadata: dict = None\n",
    "    original: str= None\n",
    "    source: str = Field(description=\"Metdata for the source document\")\n",
    "    summary: str = Field(description=\"Original content being processed\")\n",
    "    items: List[QAModel] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade5de0-2305-48c4-a4c9-1fb8a6316cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "def creat_document_lc(page_content:str, metadata:dict):\n",
    "    return Document(page_content=page_content, metadata=metadata)\n",
    "\n",
    "def format_page_content(response: QAModel):\n",
    "    return f\"\"\"Question: {response.question}\\nAnswer: {response.answer}\"\"\"\n",
    "\n",
    "def format_page_metadata(result: QADocument):\n",
    "    qaCount = 0 if not result.items else len(result.items)\n",
    "    return {\n",
    "        'source': result.source,\n",
    "        'qaCount': qaCount\n",
    "        # Additional properties here if necessary\n",
    "    }\n",
    "\n",
    "def export_document(result: QADocument) -> list :\n",
    "    metadata = format_page_metadata(result)\n",
    "    final_str = f\"***Summary***:\\n\\n{result.summary}\\n\\n***Possible QA***\\n\"\n",
    "    \n",
    "    for item in result.items:\n",
    "        qa = format_page_content(item)\n",
    "        final_str += f\"\\n{qa}\\n\"\n",
    "    \n",
    "    final_str += f\"\\n***Original***\\n\\n{result.original}\"\n",
    "    \n",
    "    doc = creat_document_lc(page_content=final_str, metadata=metadata)\n",
    "    return doc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6156af-c729-4de3-8946-ae56ea4924c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_template = \"\"\"{request}\\n{format_instructions}\"\"\"\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([human_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ef713-5194-4938-82f1-187763a3975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_request_for_questions(source_name: str, summary: str):\n",
    "    q_template1 = f\"\"\"Create a few questions and answers covering the below text starting and ending with ### :\n",
    "    ###{summary}###\n",
    "\n",
    "    Text metadata to be included in results:\n",
    "    source: {source_name}\n",
    "    summary: [This should be the text provided igenerate_request_for_questionsn above with ###]\n",
    "    original: [This can be left blank]\n",
    "    metadata: [This can be left blank]\n",
    "    \"\"\"\n",
    "\n",
    "    request = chat_prompt.format_prompt(request=q_template1,\n",
    "                                       format_instructions=parser.get_format_instructions()).to_messages()\n",
    "    \n",
    "    return request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df409ce9-c0f2-4cd3-8a33-126d9deeaaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=QADocument)\n",
    "# parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8d8213-89c8-46c8-8558-1b011130fe82",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This will genrate a Summary -> Generate Questions -> Create new document wrapper\n",
    "def process_document(document: Document) -> Document:\n",
    "    print(\"Processing document: \", document.metadata['source'])\n",
    "    \n",
    "    try:\n",
    "        # Create summary\n",
    "        doc_name = document.metadata['source']\n",
    "        summary_resp = chain.run([document])\n",
    "\n",
    "        # Create request for questions\n",
    "        question_request = generate_request_for_questions(source_name=doc_name, summary=summary_resp)\n",
    "        question_response = chat(question_request, temperature=0.0)\n",
    "        parsed_questions_qadoc = parser.parse(question_response.content)\n",
    "\n",
    "        # Set core details\n",
    "        parsed_questions_qadoc.original = document.page_content\n",
    "\n",
    "        # Generate new doc\n",
    "        final_doc = export_document(parsed_questions_qadoc)\n",
    "\n",
    "    except Exception as error:\n",
    "        # handle the exception\n",
    "        print(\"An exception occurred:\", error)\n",
    "    \n",
    "    print(\"Done processing document: \", document.metadata['source'])\n",
    "    \n",
    "    return final_doc\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d913c8-a980-4d73-8de3-f83a065ef015",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb17cef2-57c0-4a7c-82da-bb86521f7535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd792c16-4d4a-422e-b29c-4fee94a138c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# question_request[0].content\n",
    "chat(question_request[1].content, temperature=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3145cf6-e630-4dc4-9a7d-7c7f10a309df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "document = documents[1]\n",
    "\n",
    "# Create summary\n",
    "doc_name = document.metadata['source']\n",
    "summary_resp = chain.run([document])\n",
    "\n",
    "# Create request for questions\n",
    "question_request = generate_request_for_questions(source_name=doc_name, summary=summary_resp)\n",
    "question_response = chat(question_request, temperature=0.0)\n",
    "parsed_questions_qadoc = parser.parse(question_response.content)\n",
    "\n",
    "# Set core details\n",
    "parsed_questions_qadoc.original = document.page_content\n",
    "\n",
    "# Generate new doc\n",
    "final_doc = export_document(parsed_questions_qadoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424821d8-e370-4e13-9dff-d62ee1c4885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through all documents\n",
    "\n",
    "enriched_documents = []\n",
    "for doc in documents:\n",
    "    enriched = process_document(document=doc)\n",
    "    enriched_documents.append( enriched )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b36f0b-ac2f-4fec-b5e9-ceae3f29ad86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "enriched_documents[2].page_content[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188dd9b-4414-446a-9505-c5808d3e4ba4",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551435c1-2859-4b2b-857b-a7b9ca49ccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(enriched_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbca027-af80-442e-81e7-15e8b6835094",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfa405f-077f-4610-9fab-f12f2ee2a31b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67d59db-cb9b-4bcc-87e6-f5b84370cb0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "persist_dir = 'articles_db_flant5'\n",
    "\n",
    "vecordb = Chroma.from_documents(documents=texts,\n",
    "                                embedding=embedding,\n",
    "                                persist_directory=persist_dir)\n",
    "\n",
    "vecordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d0e4fe-5c0f-49e9-8014-02d6a9bc86fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = vecordb.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca6e7f-049a-4e5a-9da2-c5f084f90245",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(\"Databricks Okera\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70444df-cbdd-445b-8bcd-b77fe28fd1cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f6eedd-71d1-42c5-b14f-9c38339673e9",
   "metadata": {},
   "source": [
    "# Make Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727cba8e-f15f-4aac-b11c-4cf578b8f246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                       chain_type=\"refine\",\n",
    "                                       retriever=retriever,\n",
    "                                       return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac76b7d-e4f9-47cf-a4e9-edbfd617774e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Cite sources\n",
    "def process_llm_response(llm_response):\n",
    "    print(llm_response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bd466e-5d94-41fc-bad2-f92cb5640d8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# full example\n",
    "query = \"How much money did Pando raise?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d90a2c4-c1a0-45c8-bc5b-14b9bb889668",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# break it down\n",
    "query = \"What is the news about Pando?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)\n",
    "# llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fd6108-c894-40b0-bb81-5bb8872cb165",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Who led the round in Pando?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52d1fe3-0ffd-485c-9570-2850070b38c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What did databricks acquire?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d56d870-655c-49a9-a204-8e33083831fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is generative ai?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61d836e-9c43-47cf-870d-6249a298eb88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
